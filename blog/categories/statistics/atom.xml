<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Statistics | Piao's Blog]]></title>
  <link href="http://messipiao.github.io/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://messipiao.github.io/"/>
  <updated>2014-09-15T10:53:10+08:00</updated>
  <id>http://messipiao.github.io/</id>
  <author>
    <name><![CDATA[Chen Piao]]></name>
    <email><![CDATA[messipiao@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Asymptotic Evaluations]]></title>
    <link href="http://messipiao.github.io/blog/2014/01/14/asymptotic-evaluations/"/>
    <updated>2014-01-14T20:19:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2014/01/14/asymptotic-evaluations</id>
    <content type="html"><![CDATA[<p>All of the criteria we have considered thus far has been finite-sample criteria. In contrast, we may consider situations when the sample size becomes infinite.</p>

<h2 id="point-estimation">Point Estimation</h2>

<p>The property of consistency is a quite fundamental one of asymptotic properties, which is concerned with the asymptotic accuracy of an estimator. However, we may also consider the asymptotic variance of an estimator, called efficiency.</p>

<blockquote>
  <p>For an estimator <script type="math/tex">T_n</script>, if <script type="math/tex">% &lt;![CDATA[
lim_{n \to \infty} k_n VarT_n = \tau^2 < \infty %]]&gt;</script>, where <script type="math/tex">{k_n}</script> is a sequence of constants, then <script type="math/tex">\tau^2</script> is called the <em>limiting variance</em> or <em>limit of the variances</em>.</p>
</blockquote>

<blockquote>
  <p>For an estimator <script type="math/tex">T_n</script>, suppose that <script type="math/tex">k_n(T_n-\tau(\theta)) \to n(0,\sigma^2)</script> in distribution. The parameter <script type="math/tex">\sigma^2</script> is called the <em>asymptotic variance</em> or <em>variance of the limit distribution of $T_n$</em>.</p>
</blockquote>

<p>It is always the case that the asymptotic variance is smaller than the limiting variance, though have the same values when calculating variances of sample means and other types of average.</p>

<blockquote>
  <p>A sequence of estimators $W_n$ is asymptotically efficient for a parameter <script type="math/tex">\tau(\theta)</script> if <script type="math/tex"> \sqrt{n} [W_n-\tau(\theta)] \to n[0,v(\theta)]</script> in distribution and <script type="math/tex">v(\theta)</script> achieves the <strong>Cramer-Rao Lower Bound</strong>.</p>
</blockquote>

<p>Recall that in the above definition, </p>

<script type="math/tex; mode=display">v(\theta) = \dfrac{[\tau'(\theta)]^2}{E_{\theta}((\frac{\partial}{\partial \theta} \log f(X \vert \theta))^2)}</script>

<p>In general, we can consider MLEs to be consistent and asymptotically efficient. As a result, we can calculate the variance of MLEs approximately.</p>

<p><em>Remarks: Bootstrap is another popular way to calculate standard erors.</em></p>

<!--more-->

<h2 id="robustness">Robustness</h2>

<p>Robustness and Optimality are in the two sides of a coin. For example, generally sample mean is a more accurate estimator than sample mean, while not a robust estimator sometimes (think about adding a really huge number into a sample). M-estimators is used to consider a compromise between mean and median. We are not covering this topic here.</p>

<h2 id="hypothesis-testing">Hypothesis Testing</h2>

<p>We are going to introduce 3 large-sample tests here.</p>

<h3 id="asymptotic-distribution-of-the-lrt">Asymptotic distribution of the LRT</h3>

<blockquote>
  <p>Let <script type="math/tex">X_1,\dots,X_n</script> be a random sample from a pdf or pmf <script type="math/tex">f(x \vert \theta)</script>. Under some regularity conditions, if <script type="math/tex">\theta \in \Theta_0</script>, then the distribution of the statistic <script type="math/tex">-2\log \lambda(\mathbf{X})</script> converges to a chi squared distribution as the sample size <script type="math/tex">n \to \infty</script>. The degrees of freedom of the limiting distribution is the difference between the number of free parameters specified by <script type="math/tex">\theta \in \Theta_0</script> and the number of free parameters specified by <script type="math/tex">\theta \in \Theta</script>.</p>
</blockquote>

<h3 id="wald-test">Wald test</h3>

<p>In general, a Wald test is a test based on a satistic of the form</p>

<script type="math/tex; mode=display"> Z_n = \dfrac{W_n-\theta_0}{S_n} </script>

<p>where $\theta_0$ is a hypothesized value of the parameter $\theta$, $W_n$ is an estimator of $\theta$, and <script type="math/tex">S_n</script> is a standard error for $W_n$, an estimate of the standard deviation of $W_n$. If $W_n$ is the MLE of $\theta$, then, <script type="math/tex">1/\sqrt{I_n(W_n)}</script> is a reasonable standard error for $W_n$. Alternatively, <script type="math/tex">1/\sqrt{\hat{I}_n(W_n)}</script>, where</p>

<script type="math/tex; mode=display">\hat{I}_n(W_n) = -\frac{\partial^2}{\partial \theta^2} \log L(\theta \vert \mathbf{X}) \mid_{\theta = W_n}</script>

<p>is the observed information number, is often used. </p>

<h3 id="score-test">Score test</h3>

<p>The <em>score statistics</em> is defined to be</p>

<script type="math/tex; mode=display"> S(\theta) = \frac{\partial}{\partial \theta} \log f(\mathbf{X} \vert \theta) = \frac{\partial}{\partial \theta} \log L(\theta \vert \mathbf{X}) </script>

<p>We can obtain the result that for all $\theta$, <script type="math/tex">E_\theta S(\theta) = 0 </script>. In particular, if we are testing <script type="math/tex">H_0: \theta=\theta_0</script> and if $H_0$ is true, then <script type="math/tex">S(\theta_0)</script> has mean 0. Then we have</p>

<script type="math/tex; mode=display">Var_\theta S(\theta) = I_n(\theta)</script>

<p>The test statistic for the score test is</p>

<script type="math/tex; mode=display">Z_S = S(\theta_0)/\sqrt{I_n(\theta_0)}</script>

<p>It follows that $Z_S$ converges to a standard normal random variable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Effect]]></title>
    <link href="http://messipiao.github.io/blog/2013/12/19/random-effect/"/>
    <updated>2013-12-19T21:38:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2013/12/19/random-effect</id>
    <content type="html"><![CDATA[<p>Random effects models are often needed to account for unexplained heterogeneous situation.</p>

<p>Two commonly used distributions estimating lifetime are as follows.</p>

<h3 id="inversed-guassian-distribution">Inversed Guassian distribution</h3>

<p><script type="math/tex"> X \sim IG(\mu,\lambda) </script>, where <script type="math/tex">X \in (0,\infty)</script>. <script type="math/tex">\mu</script> and <script type="math/tex">\lambda</script> are called mean and shape parameter respectively.</p>

<ol>
  <li><em>PDF:</em> <script type="math/tex">(\frac{\lambda}{2\pi x^3})^{1/2}exp(\frac{-\lambda(x-\mu)^2}{2\mu^2 x})</script></li>
  <li><em>CDF:</em> <script type="math/tex">\Phi(\sqrt{\frac{\lambda}{x}}(\frac{x}{\mu}-1))+exp(\frac{2\lambda}{\mu})\Phi(-\sqrt{\lambda}{x}(\frac{x}{\mu}+1))</script></li>
  <li><em>Expectation and Variance:</em> <script type="math/tex">E(X)=\mu</script>, <script type="math/tex">V(X)=\frac{\mu^3}{\lambda}</script></li>
</ol>

<h3 id="gamma-distribution">Gamma distribution</h3>

<p><script type="math/tex">X \sim gamma(\alpha,\beta)</script>, where <script type="math/tex">X \in (0,\infty)</script>. <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> are called shape and rate parameter respectively.</p>

<ol>
  <li><em>PDF:</em> <script type="math/tex">\frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}</script></li>
  <li><em>Gamma function:</em> <script type="math/tex">\Gamma(t) = \int_{0}^{\infty} x^{t-1} e^{-x} dx</script></li>
  <li><em>Useful equation:</em> <script type="math/tex">\frac{\Gamma(b+1)}{a^{b+1}} = \int_{0}^{\infty} t^b e^{-at}dt</script></li>
  <li><em>Expectation and Variance:</em> <script type="math/tex">E(X)=\frac{\alpha}{\beta}</script>, <script type="math/tex">V(X)=\frac{\alpha}{\beta^2}</script></li>
</ol>

<!--more-->

<h2 id="consider-ig-processes-with-random-effects">Consider IG processes with Random Effects</h2>

<p>Consider a Wiener process <script type="math/tex">W(x) = \mu^{-1}x+\eta^{-1/2}B(x)</script> with the induced IG process <script type="math/tex">Y(t) \sim IG(\mu \Lambda(t),\eta \Lambda^2(t))</script>. A common practice to incorporate random effects in the Wiener process is to let the drift parameter <script type="math/tex">\mu^{-1}</script> vary randomly across units. Assume <script type="math/tex">\mu^{-1}</script> foolows a truncated normal distribution <script type="math/tex">TN(w,k^{-2})</script>, <script type="math/tex">k>0</script> with PDF</p>

<script type="math/tex; mode=display">g(\mu^{-1}) = \frac{k \phi[k(\mu^{-1}-w)]}{1-\Phi(-kw)}</script>

<p>the joint PDF of <script type="math/tex">\mathbf{Y}_i = [Y_i(t_{i1}),Y_i(t_{i2}),\cdots,Y_i(t_{in_i})]</script> can be computed first conditioning on the random drift parameter <script type="math/tex">\mu_i</script> and then marginalizing it</p>

<script type="math/tex; mode=display">% &lt;![CDATA[


\begin{align}
f(\mathbf{Y}_i) & = \int_{0}^{\infty} \prod_{j=1}^{n_i} \sqrt{\frac{\eta \lambda_{ij}^2}{2 \pi y_{ij}^3}} exp\{ \frac{-\eta(y_{ij}-\mu \lambda_{ij})^2}{2y_{ij}}\} \frac{k\phi[k(z-w)]}{1-\Phi(-kw)} dz \\
& = \frac{k}{1-\Phi(-kw)} \prod_{j=1}^{n_i} \sqrt{\frac{\eta \lambda_{ij}^2}{2\pi y_{ij}^3}} \int_{0}^{\infty} \prod_{j=1}^{n_i} exp \{\frac{-\eta (y_{ij}z-\lambda_{ij})^2}{2y_{ij}}\} \frac{1}{\sqrt{2\pi}} exp\{-\frac{k^2(z-w)^2}{2}\} dz \\
& = \frac{k}{1-\Phi(-kw)} \prod_{j=1}^{n_i} \sqrt{\frac{\eta \lambda_{ij}^2}{2\pi y_{ij}^3}} exp\{-\eta \sum_{j=1}^{n_i} \frac{\lambda_{ij}^2}{2y_{ij}}\} \int_{0}^{\infty} \frac{1}{\sqrt{2\pi}} exp \{-\frac{\eta}{2}Y_iz^2 + \eta \Lambda_iz-\frac{k^2(z-w)^2}{2}\} dz \\
& = \frac{1-\Phi(-\tilde{k}_i \tilde{w}_i)}{1-\Phi(-kw)} \frac{k}{\tilde{k}_i} \prod_{j=1}^{n_i} \sqrt{\dfrac{\eta \lambda_{ij}^2}{2\pi y_{ij}^3}} exp[\frac{\tilde{k}_i^2\tilde{w}_i^2-k^2w^2}{2}-\eta \sum_{j=1}^{n_i} \frac{\lambda_{ij}^2}{2y_{ij}}]
\end{align}

 %]]&gt;</script>

<p>where <script type="math/tex">y_{ij} = Y_i(t_{ij})-Y_i(t_{i,j-1})</script> is the observed increment, <script type="math/tex">\lambda_{ij} = \Lambda(t_{ij})-\Lambda(t_{i,j-1})</script>, <script type="math/tex">\tilde{k}_i = \sqrt{\eta Y_i(t_{i,n_i})+k^2}</script> and <script type="math/tex">\tilde{w}_i = (\eta \Lambda(t_{i,n_i})+wk^2)/{\tilde{k}_i^2}</script>. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypothesis Testing]]></title>
    <link href="http://messipiao.github.io/blog/2013/11/24/hypothesis-testing/"/>
    <updated>2013-11-24T10:47:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2013/11/24/hypothesis-testing</id>
    <content type="html"><![CDATA[<p>Hypothesis testing is another inference method comparing to point estimator. It is a statement about a population parameter. </p>

<blockquote>
  <p>The two complementaty hypotheses in a hypothesis testing problem are called the <em>null hypothesis</em> and the <em>alternative hypothesis</em>. They are denoted by <em>$H_0$</em> and <em>$H_1$</em>, respectively.</p>
</blockquote>

<h2 id="methods-of-finding-tests">Methods of Finding Tests</h2>

<h3 id="likelihood-ratio-tests">Likelihood Ratio Tests</h3>

<ul>
  <li>The likelihood ratio test statistic for testing <script type="math/tex">H_0:\theta \in \Theta_0</script> versus <script type="math/tex">H_1:\theta \in \Theta_0^c</script> is </li>
</ul>

<script type="math/tex; mode=display">\lambda(\mathbf{x}) = \frac{\sup_{\Theta_0} L(\theta \vert \mathbf{x})}{\sup_{\Theta} L(\theta \vert \mathbf{x})}</script>

<p>A likelihood ratio test (<strong>LRT</strong>) is any test that has a rejection region of the form <script type="math/tex">\{\mathbf{x}:\lambda(\mathbf{x}) \le c\}</script> where c is any number satisfying <script type="math/tex">0 \le c \le 1</script>.</p>

<ul>
  <li>If <script type="math/tex">T(\mathbf{X})</script> is a sufficient statistic for $\theta$ and <script type="math/tex">\lambda'(t)</script> and <script type="math/tex">\lambda(\mathbf{x})</script> are the LRT statistics based on $T$ and $\mathbf{X}$, respectively, then <script type="math/tex">\lambda'(T(\mathbf{X})) = \lambda(\mathbf{x})</script> for every $\mathbf{x}$ in the sample space.</li>
</ul>

<!--more-->

<h3 id="bayesian-tests">Bayesian Tests</h3>

<p>In a hypothesis testing problem, the posterior distribution may be used to calculate the probabilities that $H<em>0$ and $H</em>1$ are true. Remember, <script type="math/tex">\pi(\theta \vert \mathbf{x})</script> is probability distribution for a random variable. Hence, the posterior probabilities <script type="math/tex">P(\theta \in \Theta_0 \vert \mathbf{x}) = P(H_0 \text{is true} \vert \mathbf{x})</script> and <script type="math/tex">P(\theta \in \Theta^c_0 \vert \mathbf{x}) = P(H_1 \text{is true} \vert \mathbf{x})</script> may be computed. </p>

<p>One way a Bayesian hypothesis tester may choose to use the posterior distribution is to decide to accept <script type="math/tex">H_0</script> as true if <script type="math/tex">P(\theta \in \Theta_0 \vert \mathbf{X}) \ge P(\theta \in \Theta^c_0 \vert \mathbf{X})</script> and to reject <script type="math/tex">H_0</script> otherwise. Another way is to reject <script type="math/tex">H_0</script> only if <script type="math/tex">P(\theta \in \Theta_0^c \vert \mathbf{X})</script> is greater thatn some large number, 0.99 for example.</p>

<h3 id="union-intersection-and-intersection-union-tests">Union-Intersection and Intersection-Union Tests</h3>

<ul>
  <li><strong>Union-Intersection method</strong></li>
</ul>

<script type="math/tex; mode=display">H_0: \theta \in \bigcap_{\gamma \in \Gamma} \Theta_{\gamma}</script>

<p>Then the rejection region for the union-intersection test is</p>

<script type="math/tex; mode=display">\bigcup_{\gamma \in \Gamma} \{\mathbf{x}:T_{\gamma}(\mathbf{x}) \in R_{\gamma}\}</script>

<ul>
  <li><strong>Intersection-Union method</strong></li>
</ul>

<script type="math/tex; mode=display">H_0: \theta \in \bigcup_{\gamma \in \Gamma} \Theta_{\gamma}</script>

<p>Then the rejection region for the intersection-union test is</p>

<script type="math/tex; mode=display">\bigcap_{\gamma \in \Gamma} \{\mathbf{x}:T_{\gamma}(\mathbf{x}) \in R_{\gamma}\}</script>

<h2 id="methods-of-evaluating-tests">Methods of Evaluating Tests</h2>

<h3 id="power-function">Power Function</h3>

<p>A hypothesis test might make one of two types of errors, namely Type I Error and Type II Error. If <script type="math/tex">\theta \in \Theta_0</script> but the hypothesis test incorrectly decides to reject <script type="math/tex">H_0</script>, then the test has made a Type I Error. If, on the other hand, <script type="math/tex">\theta \in \Theta_0^c</script> but the test decides to accept <script type="math/tex">H_0</script>, a Type II Eroror has been made.</p>

<blockquote>
  <p>The power function of a hypothesis test with rejection region R is the function of $\theta$ defined by <strong><script type="math/tex">\beta(\theta) = P_{\theta}(\mathbf{X} \in R)</script></strong>.</p>
</blockquote>

<p>For a fixed sample size, it is usually impossible to make both types of error probabilities arbitrarily small. In searching for a good test, it is common to restrict consideration to tests that control the Type I Error probability at a specified level. Within this class of tests we then search for tests that have Type II Error probability that is as small as possible. The following two terms are useful when discussing tests that control Type I Error probabilities.</p>

<ul>
  <li>For <script type="math/tex">0 \le \alpha \le 1</script>, a test with power function <script type="math/tex">\beta(\theta)</script> is a size $\alpha$ test if <script type="math/tex">\sup_{\theta \in \Theta_0} \beta(\theta) = \alpha</script>.</li>
  <li>For <script type="math/tex">0 \le \alpha \le 1</script>, a test with power function <script type="math/tex">\beta(\theta)</script> is a level $\alpha$ test if <script type="math/tex">\sup_{\theta \in \Theta_0} \beta(\theta) \le \alpha</script>.</li>
</ul>

<blockquote>
  <p>A test with power function <script type="math/tex">\beta(\theta)</script> is <strong>unbiased</strong> if <script type="math/tex">\beta(\theta') \ge \beta(\theta'')</script> for every <script type="math/tex">\theta' \in \Theta_0^c</script> and <script type="math/tex">\theta'' \in \Theta_0</script>.</p>
</blockquote>

<h3 id="most-powerful-tests">Most Powerful Tests</h3>

<blockquote>
  <p>Let C be a class of tests for testing <script type="math/tex">H_0:\theta \in \Theta_0</script> versus <script type="math/tex">H_1 : \theta \in \Theta_0^c</script>. A test in class C, with power function <script type="math/tex">\beta(\theta)</script>, is a <em>uniformly most powerful</em>(<strong>UMP</strong>) class C test if <script type="math/tex">\beta(\theta) \ge \beta'(\theta)</script> for every <script type="math/tex">\theta \in \Theta^c_0</script> and every <script type="math/tex">\beta'(\theta)</script> that is a power function of a test in class C.</p>
</blockquote>

<p>If the class C is the class of all level $\alpha$ tests, then the test described above is called a <strong>UMP level $\alpha$ test</strong>.</p>

<p><strong>Neyman-Pearson Lemma</strong> describes which tests are UMP level $\alpha$ tests in the situation where the null and alternative hypotheses both consist of only one probability distribution for the sample (that is, when both <script type="math/tex">H_0</script> and <script type="math/tex">H_1</script> are simple hypotheses).</p>

<ul>
  <li><strong>Neyman-Pearson Lemma</strong></li>
</ul>

<p>Consider testing <script type="math/tex">H_0:\theta = \theta_0</script> versus <script type="math/tex">H_1:\theta = \theta_1</script>, where the pdf or pmf corresponding to $\theta_i$ is <script type="math/tex">f(\mathbf{x} \vert \theta_i)</script>, <script type="math/tex">i=0,1</script>, using a test with rejection region R that satisfies</p>

<script type="math/tex; mode=display">\mathbf{x} \in R ~ if ~ f(\mathbf{x} \vert \theta_1) > kf(\mathbf{x} \vert \theta_0)</script>

<p>(1) and</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\mathbf{x} \in R^c ~ if ~ f(\mathbf{x} \vert \theta_1) < kf(\mathbf{x} \vert \theta_0) %]]&gt;</script>

<p>for some <script type="math/tex">k \ge 0</script>, and </p>

<p>(2) </p>

<script type="math/tex; mode=display">\alpha = P_{\theta_0}(\mathbf{X} \in R)</script>

<p>Then</p>

<ol>
  <li><strong>(Sufficiency)</strong> Any test that satisfies (1) and (2) is a <strong>UMP level $\alpha$ test</strong>.</li>
  <li><strong>(Necessity)</strong> If there exists a test satisfying (1) and (2) with <script type="math/tex">k > 0</script>, then every <strong>UMP level $\alpha$ test</strong> is a size $\alpha$ test (satisfies (2)) and every <strong>UMP level $\alpha$ test</strong> satisfies (1) except perhaps onon a set A satisfying <script type="math/tex">P_{\theta_0}(\mathbf{X} \in A) = P_{\theta_1}(\mathbf{X} \in A) = 0</script>.</li>
</ol>

<p>Note that replace $\mathbf{x}$ by a sufficient statistic $T(\mathbf{x})$ can lead to a same conclusion(applying <strong>Factorization Theorem</strong>).</p>

<p>Hypotheses, such as <script type="math/tex">H_0</script> and <script type="math/tex">H_1</script> in the <strong>Neyman-Pearson Lemma</strong>, that specify only one possible distribution for the sample $\mathbf{X}$ are called <em>simple hypotheses</em>. In most realistic problems, the hypotheses of interest specify more than one possible distribution for the sample. Such hypotheses are called <em>composite hypothese</em>. </p>

<ul>
  <li><script type="math/tex">H:\theta \ge \theta_0</script> or <script type="math/tex">% &lt;![CDATA[
\theta < \theta_0 %]]&gt;</script> is called <em>one-sided</em> hypotheses.</li>
  <li><script type="math/tex">H:\theta \ne \theta_0</script> is called <em>two-sided</em> hypotheses.</li>
</ul>

<p>A large class of problems that admit <strong>UMP level $\alpha$</strong> tests involve one-sided hypotheses and <strong>pdfs or pmfs</strong> with the <strong>monotone likelihood ratio property</strong>.</p>

<blockquote>
  <p>A family of pdfs or pmfs <script type="math/tex">\{g(t \vert \theta):\theta \in \Theta\}</script> for a univariate random variable T with real-valued parameter $\theta$ has a <em>monotone likelihood ratio</em> (<strong>MLR</strong>) if, for every <script type="math/tex">\theta_2 > \theta_1</script>, <script type="math/tex">g(t \vert \theta_2)/g(t \vert \theta_1)</script> is a monotone (nonincreasing or nondecreasing) function of t on <script type="math/tex">\{t:g(t \vert \theta_1)>0 ~or~ g(t \vert \theta_2)>0\}</script>. Note that c/0 is defined as $\infty$ if $c&gt;0$.</p>
</blockquote>

<ul>
  <li>Many common families of distribution have an <em>MLR</em>. For example, the normal (known variance, unknown mean). Poisson, and binomial all have an MLR. </li>
  <li>
    <p>Any regular exponential family with <script type="math/tex">g(t \vert \theta) = h(t)c(\theta) e^{w(\theta)t}</script> has an <em>MLR</em> if <script type="math/tex">w(\theta)</script> is a nondecreasing function.</p>
  </li>
  <li><strong>Karlin-Rubin Theorem</strong></li>
</ul>

<p>Consider testing <script type="math/tex">H_0:\theta \le \theta_0</script> versus <script type="math/tex">H_1: \theta > \theta_0</script>. Suppose that T is a sufficient statistic for $\theta$ and the family of pdfs or pmfs <script type="math/tex">\{g(t \vert \theta):\theta \in \Theta\}</script> of T has an <em>MLR</em>. Then for any <script type="math/tex">t_0</script>, the test that rejects <script type="math/tex">H_0</script> if and only if <script type="math/tex">T > t_0</script> is a <strong>UMP level $\alpha$ test</strong>, where <script type="math/tex">\alpha = P_{\theta_0}(T>t_0)</script>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Point Estimation]]></title>
    <link href="http://messipiao.github.io/blog/2013/11/22/point-estimation/"/>
    <updated>2013-11-22T19:31:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2013/11/22/point-estimation</id>
    <content type="html"><![CDATA[<p>When sampling is from a population described by a pdf or pmf <script type="math/tex">f(x \vert \theta)</script>, knowledge of $\theta$ yields knowledge of the entire population. Hence, it is natural to seek a method of finding a good estimator of the point $\theta$, that is, a good point estimator. Note that an estimator is a function of the sample while an estimate is the realized value of an estimator (that is, a number) that is obtained when a sample is actually taken.</p>

<h2 id="methods-of-finding-estimators">Methods of Finding Estimators</h2>

<h3 id="method-of-moments">Method of Moments</h3>

<p>Let <script type="math/tex">X_1,\dots,X_n</script> be a sample from a population with pdf or pmf <script type="math/tex">f(x \vert \theta_1,\dots,\theta_k)</script>. Method of moments estimators are found by equating the first k sample moments to the corresponding k population moments.</p>

<script type="math/tex; mode=display">m_k = \frac{1}{n} \sum_{i=1}^{n}X_i^k = \mu'_k = EX^k</script>

<h3 id="maximum-likelihood-estimators">Maximum Likelihood Estimators</h3>

<p>The likelihood function is defined by</p>

<script type="math/tex; mode=display">L(\mathbf{\theta} \vert \mathbf{x}) = L(\theta_1,\dots,\theta_k \vert x_1,\dots,x_n) = \prod_{i=1}^{n} f(x_i \vert \theta_1,\dots,\theta_k)</script>

<p>Basicly, we can solve the first derivative of <em>log likelihood function</em> to get the MLEs. If the likelihood function cannot be maximized analytically, it may be possible to use a computer and maximize the likelihood function numerically.</p>

<p>MLE has the following properties.</p>

<ul>
  <li><strong>(Invariance property of MLEs)</strong> </li>
</ul>

<p>If $\hat{\theta}$ is the MLE of $\theta$, then for any function $f(\theta)$, the MLE of $f(\theta)$ is $f(\hat{\theta})$.</p>

<ul>
  <li><strong>(Asymptotic Normality)</strong> </li>
</ul>

<p>Under appropriate regularity conditions,</p>

<script type="math/tex; mode=display">\sqrt{I_n(\theta_0)} (\hat{\theta}_n-\theta_0) \sim N(0,1)</script>

<p>In addition</p>

<script type="math/tex; mode=display">\sqrt{I_n(\hat{\theta}_n)} (\hat{\theta}_n-\theta_0) \sim N(0,1)</script>

<p>where $I(\theta)$ is called the information number or <em>Fisher information</em> of the sample.</p>

<script type="math/tex; mode=display"> I(\theta) = E[(\frac{\partial}{\partial \theta} \log f(X;\theta))^2] = -E[\frac{\partial^2}{\partial \theta^2} \log f(X;\theta)]</script>

<p>Fisher information has following properties.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align}
1. & I_{X,Y}(\theta) = I_X(\theta)+I_Y(\theta)\\
2. & I_n(\theta) = n I(\theta) \\
\end{align}
 %]]&gt;</script>

<!--more-->

<h3 id="the-em-algorithm">The EM Algorithm</h3>

<p>The EM algorithm is used to find <strong>MLEs</strong>, which is guaranteed to converge to the MLE. It is particularly suited to “missing data” problems, as the very fact that there are missing data can sometimes make calculations cumbersome.</p>

<p>If <script type="math/tex">\mathbf{Y} = (Y_1,\dots,Y_N)</script> are the incomplete data, and <script type="math/tex">\mathbf{X} = (X_1,\dots,X_m)</script> are the augmented data (missing data), making <script type="math/tex">(\mathbf{Y},\mathbf{X})</script> the complete data. The densities $g(\cdot \vert \theta)$ of $\mathbf{Y}$ and $f(\cdot \vert \theta)$ of $\mathbf{Y},\mathbf{X}$ have the relationship</p>

<script type="math/tex; mode=display">g(\mathbf{y} \vert \theta) = \int f(\mathbf{y},\mathbf{x} \vert \theta) dx</script>

<p>If we turn these into the likelihoods, <script type="math/tex">L(\theta \vert \mathbf{y}) = g(\mathbf{y} \vert \theta)</script> is the incomplete-data likelihood and <script type="math/tex">L(\theta \vert \mathbf{y},\mathbf{x}) = f(\mathbf{y},\mathbf{x} \vert \theta)</script> is the complete-data likelihood.i</p>

<p>Define</p>

<script type="math/tex; mode=display">k(\mathbf{x} \vert \theta,\mathbf{y}) = \frac{f(\mathbf{y},\mathbf{x} \vert \theta)}{g(\mathbf{y} \vert \theta)} = \frac{L(\theta \vert \mathbf{y},\mathbf{x})}{L(\theta \vert \mathbf{y})}</script>

<p>Then we have</p>

<script type="math/tex; mode=display">\log L(\theta \vert \mathbf{y}) = \log L(\theta \vert \mathbf{y},\mathbf{x}) - \log k(\mathbf{x} \vert \theta,\mathbf{y})</script>

<p>replace the right side with its expectation under $k(\mathbf{x} \vert \theta’,\mathbf{y})$, we have</p>

<script type="math/tex; mode=display">\log L(\theta \vert \mathbf{y}) = E[\log L(\theta \vert \mathbf{y},\mathbf{x}) \vert \theta',\mathbf{y}] - E[\log k(\mathbf{x} \vert \theta,\mathbf{y}) \vert \theta',\mathbf{y}]</script>

<p>Now we start the algorithm: From an initial value <script type="math/tex">\theta^{(0)}</script> we create a sequence <script type="math/tex">\theta^{(r)}</script> according to</p>

<script type="math/tex; mode=display">\theta^{(r+1)} = \max_{\theta} E[\log L(\theta \vert \mathbf{y},\mathbf{x}) \vert \theta^{(r)},\mathbf{y}] </script>

<h3 id="bayes-estimators">Bayes Estimators</h3>

<p>In Bayes approach $\theta$ is considered to be a quantity whose variation can be described by a probability distribution (called the prior distribution). This is a subjective distribution, based on the experimenter’s belief, and is formulated before the data are seen (hence the name prior distribution). A sample is then taken from a population indexed by $\theta$ and the prior distribution is updated with this sample information. The updated prior is called the posterior distribution. </p>

<p>If we denote the prior distribution by $\pi(\theta)$ and the sampling distribution by <script type="math/tex">f(x \vert \theta)</script>, then the posterior distribution, the conditional distribution of $\theta$ given the sample, <script type="math/tex">\mathbf{x}</script>, </p>

<script type="math/tex; mode=display">\pi(\theta \vert \mathbf{x}) = f(\mathbf{x} \vert \theta) \pi(\theta)/m(\mathbf{x})</script>

<p>where $m(\mathbf{x})$ is the marginal distribution of $\mathbf{X}$, that is,</p>

<script type="math/tex; mode=display">m(\mathbf{x}) = \int f(\mathbf{x} \vert \theta) \pi(\theta) d \theta</script>

<h2 id="methods-of-evaluating-estimators">Methods of Evaluating Estimators</h2>

<h3 id="mean-squared-error">Mean Squared Error</h3>

<blockquote>
  <p>The mean square error (MSE) of an estimator W of a parameter $\theta$ is the function of $\theta$ defined by $E_{\theta}(W-\theta)^2$.</p>
</blockquote>

<p>Define</p>

<script type="math/tex; mode=display">Bias_{\theta}W = E_{\theta}W-\theta</script>

<p>Then</p>

<script type="math/tex; mode=display">E_{\theta}(W-\theta)^2 = Var_{\theta}W+(E_{\theta}W-\theta)^2 = Var_{\theta}W+(Bias_{\theta}W)^2</script>

<h3 id="best-unbiased-estimators">Best Unbiased Estimators</h3>

<blockquote>
  <p>An estimator W’ is a <em>best unbiased estimator of $\tau(\theta)$</em> if it satisfies <script type="math/tex">E_{\theta}W' = \tau(\theta)</script> for all $\theta$ and, for any other estimator W with <script type="math/tex">E_{\theta}W = \tau(\theta)</script>, we have <script type="math/tex">Var_{\theta}W' \le Var_{\theta}W</script> for all $\theta$. W’ is also called a <em>uniform minimum variance unbiased estimator</em> (<strong>UMVUE</strong>) of $\tau(\theta)$.</p>
</blockquote>

<ul>
  <li><strong>Cramer-Rao Inequality</strong></li>
</ul>

<p>Let <script type="math/tex">X_1,\dots,X_n</script> be a sample with pdf $f(x \vert \theta)$, and let <script type="math/tex">W(\mathbf{X}) = W(X_1,\dots,X_n)</script> be <em>any estimator satisfying</em></p>

<script type="math/tex; mode=display">\frac{d}{d\theta} E_{\theta}W(\mathbf{X}) = \int \frac{\partial}{\partial \theta}[W(\mathbf{x})f(x \vert \theta)] dx</script>

<p>and</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
Var_{\theta}W(\mathbf{X}) < \infty %]]&gt;</script>

<p>Then</p>

<script type="math/tex; mode=display">Var_{\theta}(W(\mathbf{X})) \ge \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{E_{\theta}[(\frac{\partial}{\partial \theta} \log f(\mathbf{X} \vert \theta))^2]} = \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{I_n(\theta)}</script>

<p>when <script type="math/tex">X_1,\dots,X_n</script> are <em>iid</em>, based on the property of <em>Fisher Information</em>, we have</p>

<script type="math/tex; mode=display">Var_{\theta}(W(\mathbf{X})) \ge \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{nE_{\theta}[(\frac{\partial}{\partial \theta} \log f(X \vert \theta))^2]} = \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{nI(\theta)}</script>

<p>We need to verify whether the <strong>Cramer-Rao Lower Bound</strong> is attainable.</p>

<ul>
  <li>Let <script type="math/tex">X_1,\dots,X_n</script> be <em>iid</em> $f(x \vert \theta)$, which satisfies the conditions of the <em>Cramer-Rao Theorem</em>. Let <script type="math/tex">L(\theta \vert \mathbf{x}) = \prod_{i=1}^{n} f(x_i \vert \theta)</script> denote the likelihood function. If $$
W(\mathbf{X})=W(X_1,\dots,X_n) $$ is any unbiased estimator of $\tau(\theta)$, then $W(\mathbf{X})$ attains the Cramer-Rao Lower Bound if and only if </li>
</ul>

<script type="math/tex; mode=display">a(\theta)[W(\mathbf{x})-\tau(\theta)] = \frac{\partial}{\partial \theta}\log L(\theta \vert \mathbf{x})</script>

<p>for some function $a(\theta)$.</p>

<p>Two theorems are used to find the <strong>UMVUE</strong>.</p>

<ul>
  <li><strong>Rao-Blackwell Theorem</strong> </li>
</ul>

<p>Let W be any unbiased estimator of $\tau(\theta)$, and let T be a sufficient statistic for $\theta$. Then $E(W \vert T)$ is a UMVUE.</p>

<ul>
  <li><strong>Lehmann Scheffe Theorem</strong></li>
</ul>

<p>Suppose there exists a sufficient and complete statistic T for $\theta$. If there exisits unbiased estimator of $\tau(\theta)$, then the UMVUE takes the form of h(T), where h is a Borel function.</p>

<p>It is also noted that UMVUE is <strong>unique</strong>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Reduction]]></title>
    <link href="http://messipiao.github.io/blog/2013/11/22/data-reduction/"/>
    <updated>2013-11-22T16:23:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2013/11/22/data-reduction</id>
    <content type="html"><![CDATA[<p>Any statistic, T(<strong>X</strong>), defines a form of data reduction or data summay. </p>

<h2 id="sufficient-statistics">Sufficient Statistics</h2>

<blockquote>
  <p>A statistic T(<strong>X</strong>) is a sufficient statistic for <script type="math/tex">\theta</script> if the conditional distribution of the samle <strong>X</strong> given the value of T(<strong>X</strong>) does not depend on $\theta$.</p>
</blockquote>

<p>A sufficient statistic captures all the infromation about $\theta$.</p>

<blockquote>
  <p>If <script type="math/tex">p( \mathbf{x} \vert \theta)</script> is the joint pdf or pmf of <script type="math/tex">\mathbf{X}</script> and <script type="math/tex">q(t \vert \theta)</script> is the pdf or pmf of <script type="math/tex">T(\mathbf{X})</script>, then <script type="math/tex">T(\mathbf{X})</script> is a sufficient statistic for $\theta$ if, for every x in the sample space, the ration <script type="math/tex">p(\mathbf{x} \vert \theta)/q(T(\mathbf{x}) \vert \theta)</script> is constant as a function of $\theta$.</p>
</blockquote>

<p>To use the definition, we must guess a statistic $T(\mathbf{X})$ to be sufficient, find the pmf of pdf of $T(\mathbf{X})$, and check that the ratio of pdfs or pmfs does not depend on $\theta$. <strong>Factorization Theorem</strong> provides us another way to find a sufficient statistic.</p>

<ul>
  <li>Lef <script type="math/tex">f(\mathbf{x}\vert \theta)</script> denote the joint pdf or pmf of a sample <script type="math/tex">\mathbf{X}</script>. A statistic $T(\mathbf{X})$ is a sufficient statistic for $\theta$ if and only if there exist functions <script type="math/tex">g(t \vert \theta)</script> and <script type="math/tex">h(\mathbf{x})</script> such that, for all sample points $\mathbf{x}$ and all parameter points $\theta$,</li>
</ul>

<script type="math/tex; mode=display">f(\mathbf{x} \vert \theta) = g(T(\mathbf{x}) \vert \theta)h(\mathbf{x})</script>

<p>It is easy to find a sufficient statistic for an exponential family of ditributions using the <strong>Factorizaion Theorem</strong>.</p>

<ul>
  <li>Let <script type="math/tex">X_1,\dots,X_n</script> be <em>iid</em> observations from a pdf or pfm <script type="math/tex">f(x \vert \mathbf{\theta})</script> that belongs to an exponential family given by</li>
</ul>

<script type="math/tex; mode=display">f(x \vert \mathbf{\theta}) = h(x)c(\mathbf{\theta})exp(\sum_{i=1}^{k}w_i(\mathbf{\theta})t_i(x)))</script>

<p>where <script type="math/tex">\mathbf{\theta} = (\theta_1,\dots,\theta_d)</script>, <script type="math/tex">d \le k</script>. Then</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (\sum_{j=1}^{n}t_1(X_j),\dots,\sum_{j=1}^{n}t_k(X_j)))</script>

<p>is a sufficient statistic for <script type="math/tex">\theta</script>.</p>

<!--more-->

<h2 id="minimal-sufficient-statistics">Minimal Sufficient Statistics</h2>

<p>A minimal sufficient statistic achieves the most data reduction while still retaining all the information about $\theta$.</p>

<blockquote>
  <p>A sufficient statistic <script type="math/tex">T(\mathbf{X})</script> is called a minimal sufficient statistic if, for any other sufficient statistic <script type="math/tex">T'(\mathbf{X})</script>, <script type="math/tex">T(\mathbf{X})</script> is a function of <script type="math/tex">T'(\mathbf{X})</script>.</p>
</blockquote>

<p>We have an easier way to find a minimal sufficient statistic.</p>

<ul>
  <li>Let <script type="math/tex">f(\mathbf{x}\vert \theta)</script> be the pmf or pdf of a sample $\mathbf{X}$. Suppose there exists a function <script type="math/tex">T(\mathbf{x})</script> such that, for every two sample points <strong>x</strong> and <strong>y</strong>, the ratio <script type="math/tex">f(\mathbf{x} \vert \theta)/f(\mathbf{y} \vert \theta)</script> is constant as a function of $\theta$ if and only if <script type="math/tex">T(\mathbf{x}) = T(\mathbf{y})</script>. Then <script type="math/tex">T(\mathbf{X})</script> is a minimal sufficient statistic for $\theta$.</li>
</ul>

<p>Note that a minimal sufficient statistic is not unique. Any <strong>one-to-one</strong> function of a minimal sufficient statistic is also a minimal sufficient statistic.</p>

<h2 id="ancillary-statistics">Ancillary Statistics</h2>

<blockquote>
  <p>A stastistic <script type="math/tex">S(\mathbf{X})</script> whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic.</p>
</blockquote>

<p><strong>Alone</strong>, an ancillary statistic contains no information about $\theta$.</p>

<ul>
  <li>Location family ancillary statistic: Let <script type="math/tex">X_1,\dots,X_n</script> be <em>iid</em> observations from a location parameter family with cdf $F(x-\theta)$, <script type="math/tex">% &lt;![CDATA[
-\infty < \theta < \infty %]]&gt;</script>. <script type="math/tex">R = X_{(n)}-X_{(1)}</script> is an ancillary statistic.</li>
  <li>Scale family ancillary statistic: Let <script type="math/tex">X_1,\dots,X_n</script> be <em>iid</em> observations from a scale parameter family with cdf $F(x/\sigma)$, $\sigma &gt;0$. Then any statistic that depends on the sample only through the $n-1$ values <script type="math/tex">X_1/X_n,\dots,X_{n-1}/X_n</script> is an ancillary statistic.</li>
</ul>

<p>The knowlede of ancillary statistics alone give us no information about $\theta$, while it may increse our knowledge about $\theta$ (can be a part of a sufficient statistic). For many important situations, our intuition that a minimal sufficient statistic is independent of any ancillary statistic is correct. </p>

<h2 id="complete-statistics">Complete Statistics</h2>

<blockquote>
  <p>Let <script type="math/tex">f(t \vert \theta)</script> be a family of pdfs or pmfs for a statistic <script type="math/tex">T(\mathbf{X})</script>. The family of probability distributions is called complete if <script type="math/tex">E_{\theta}g(T) = 0</script> for all $\theta$ implies <script type="math/tex">P_{\theta}(g(T)=0) = 1</script> for all $\theta$. Equivalently, <script type="math/tex">T(\mathbf{X})</script> is called a complete statistic.</p>
</blockquote>

<p>Informally, a statistic $T(\mathbf{X})$ is complete if two different parameters $\theta$ of the distribution of $\mathbf{X}$, cannot give rise to the same distribution for $T\mathbf{X}$.</p>

<p>We can get the complete statistics in the exponential family.</p>

<ul>
  <li>Let <script type="math/tex">X_1,\dots,X_n</script> be <em>iid</em> observations from a pdf or pfm <script type="math/tex">f(x \vert \mathbf{\theta})</script> that belongs to an exponential family given by</li>
</ul>

<script type="math/tex; mode=display">f(x \vert \mathbf{\theta}) = h(x)c(\mathbf{\theta})exp(\sum_{i=1}^{k}w_i(\mathbf{\theta})t_i(x)))</script>

<p>where <script type="math/tex">\mathbf{\theta} = (\theta_1,\dots,\theta_d)</script>, <script type="math/tex">d \le k</script>. Then</p>

<script type="math/tex; mode=display">T(\mathbf{X}) = (\sum_{j=1}^{n}t_1(X_j),\dots,\sum_{j=1}^{n}t_k(X_j)))</script>

<p>is complete if <script type="math/tex">\{(w_1(\theta),\dots,w_k(\theta)):\theta \in \Theta\}</script> contains an open set in $R^k$.</p>

<p>We use completeness to state a condition under which a minimal sufficient statistic is independent of every ancillary statistic.</p>

<ul>
  <li><strong>(Basu’s Theorem)</strong> If <script type="math/tex">T(\mathbf{X})</script> is complete and minimal sufficient statistic, then $T(\mathbf{X})$ is independent of every ancillary statistic. </li>
</ul>

<p>It is noted that the theorem is also true if we omit the minimality constraint, as we have</p>

<ul>
  <li><em>If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistc.</em> </li>
</ul>

<h2 id="applications-and-special-distributions">Applications and Special distributions</h2>

<ul>
  <li><strong>The Bernoulli Distribution</strong></li>
</ul>

<p>$Y=\sum X_i$ is minimal sufficient and complete for $p$.</p>

<ul>
  <li><strong>Poisson Distribution</strong></li>
</ul>

<p>$Y = \sum X_i$ is minimal sufficient and complete for $\theta$.</p>

<ul>
  <li><strong>The Uniform Distribution</strong></li>
</ul>

<ol>
  <li>interval $[0,\theta]$, $X_{(n)}$ is sufficient for $\theta$;</li>
  <li>interval <script type="math/tex">[\theta,\theta+1]</script>, <script type="math/tex">(X_{(1)},X_{(n)})</script> is minimal sufficient for $\theta$.</li>
</ol>

<ul>
  <li><strong>Normal Distribution</strong></li>
</ul>

<ol>
  <li><script type="math/tex">(\sum X_i,\sum X^2_i)</script> is sufficient for <script type="math/tex">(\mu,\sigma^2)</script>;</li>
  <li><script type="math/tex">(\bar{X},S^2)</script> is sufficient for <script type="math/tex">(\mu,\sigma^2)</script>.</li>
</ol>

]]></content>
  </entry>
  
</feed>
