<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Statistics | Piao's Blog]]></title>
  <link href="http://messipiao.github.io/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://messipiao.github.io/"/>
  <updated>2013-11-20T17:12:16+08:00</updated>
  <id>http://messipiao.github.io/</id>
  <author>
    <name><![CDATA[Chen Piao]]></name>
    <email><![CDATA[messipiao@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Random Sample]]></title>
    <link href="http://messipiao.github.io/blog/2013/11/20/random-sample/"/>
    <updated>2013-11-20T15:03:00+08:00</updated>
    <id>http://messipiao.github.io/blog/2013/11/20/random-sample</id>
    <content type="html"><![CDATA[<h2 id="basic-concepts-of-random-samples">Basic Concepts of Random Samples</h2>

<blockquote>
  <p>The random variables <script type="math/tex">X_1,\dots,X_n</script> are called a random sample of size n from the population <script type="math/tex">f(x)</script> if <script type="math/tex">X_1,\dots,X_n</script> are mutually independent random variables and the marginal pdf or pmf of each <script type="math/tex">X_i</script> is the same function <script type="math/tex">f(x)</script>. Alternatively, <script type="math/tex">X_1,\dots,X_n</script> are called independent and identically distributed random variables with pdf or pmf <script type="math/tex">f(x)</script>. This is commonly abbreviated to iid random variables. </p>
</blockquote>

<p>The joint pdf of pmf of <script type="math/tex">X_1,\dots,X_n</script> is given by </p>

<script type="math/tex; mode=display">f(x_1,\dots,x_n) = f(x_1)f(x_2)\cdots f(x_n) = \prod_{i=1}^{n}f(x_i)</script>

<p>Two methods, sampling with and without replacement for drawing a random sample are considered here. When sampling is from a infinite population, both methods make the equation hold, as removing any sample will not change the population. However, when sampling is from a finite population, sampling with replacement still makes the equation holds while sampling without replacement not. This is because the former method will not change the population, so the random variable <script type="math/tex">X_1,\dots,X_n</script> are independent as the process of choosing any <script type="math/tex">X_i</script> keep the same. As for method of sampling without replacement, the probability distribution for $X_j$ depneds on the value of <script type="math/tex">X_i</script>, where <script type="math/tex">j > i</script>. For example, considering <script type="math/tex">X_1</script> and <script type="math/tex">X_2</script>, we have</p>

<script type="math/tex; mode=display">P(X_2 = x_1 \mid X_1=x_1) = 0  ~ \text{and} ~P(X_2=x \mid X_1=x_1) = \dfrac{1}{N-1} ~ \text{for} ~ x \ne x_1</script>

<p>It is noted that <script type="math/tex">X_i</script> has the same marginal distribution.</p>

<script type="math/tex; mode=display">P(X_2=x) = \sum_{i=1}^{N} P(X_2=x \mid X_1=x_i)P(X_1=x_i) = \frac{1}{N}</script>

<p>When $N$ is larger, there is not too difference between the conditional distribution of <script type="math/tex">X_i</script> given <script type="math/tex"> X_1, \dots,X_{i-1} </script> and the marginal distribution. We say the random variables are <em>nearly independent</em>. </p>

<h2 id="convergence">Convergence</h2>

<h3 id="types-of-convergence">Types of convergence</h3>

<ul>
  <li><strong>Convergence in Probability</strong></li>
</ul>

<p>A sequence of random variables, <script type="math/tex">X_1,X_2,\dots</script>, converges in probability to a random variable $X$ if, for every <script type="math/tex">\epsilon > 0</script>,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\lim_{n \to \infty} P(\mid X_n-X \mid \ge \epsilon (\text{or} ~~ < \epsilon)) = 0 (\text{or} ~~ 1) %]]&gt;</script>

<ul>
  <li><strong>Convergence in almost sure</strong></li>
</ul>

<p>A sequence of random variables, <script type="math/tex">X_1,X_2,\dots</script>, converges in probability to a random variable $X$ if, for every <script type="math/tex">\epsilon > 0</script>,</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
P(\lim_{n \to \infty} \mid X_n-X \mid < \epsilon ) = 1 %]]&gt;</script>

<p>The above definition is much stronger than that of convergence in probability.</p>

<ul>
  <li><strong>Convergence in Distribution</strong></li>
</ul>

<p>A sequence of random variables, <script type="math/tex">X_1,X_2,\cdots</script> , converge in distribution to a random variable $X$
If</p>

<script type="math/tex; mode=display">\lim_{n \to \infty} F_{X_n}(x) = F_X(x)</script>

<p>at all points x there <script type="math/tex">F_X(x)</script> is continuous.</p>

<h3 id="related-theorem-or-method">Related Theorem or Method</h3>

<ul>
  <li><strong>Continuous mapping Theorem</strong></li>
</ul>

<p>If $X_n$ converges in probability or almost sure or distribution to a random variable $X$, and <script type="math/tex">g(x)</script> is a continuous function, then <script type="math/tex">g(X_n)</script> converges in corresponding form to <script type="math/tex">g(X)</script>.</p>

<ul>
  <li><strong>Slutsky’s Theorem</strong></li>
</ul>

<p>If <script type="math/tex">X_n \to X</script> in distribution and <script type="math/tex">Y_n \to a</script>, a costant, in probability then</p>

<ol>
  <li><script type="math/tex">Y_nX_n \to aX</script> in distribution.</li>
  <li><script type="math/tex">X_n+Y_n \to X+a</script> in distribution.</li>
</ol>

<ul>
  <li><strong>Weak Law of Large Numbers</strong></li>
</ul>

<p>If $X_n$ are i.i.d distributed with <script type="math/tex">% &lt;![CDATA[
E\mid X_n \mid < \infty %]]&gt;</script>, <script type="math/tex">\mu = EX_n</script>,then</p>

<script type="math/tex; mode=display">\bar{X} \to \mu \quad \text{in probability}</script>

<ul>
  <li><strong>Strong Law of Large Numbers</strong></li>
</ul>

<p>If $X_n$ are i.i.d distributed with <script type="math/tex">% &lt;![CDATA[
E\mid X_n \mid < \infty %]]&gt;</script>, <script type="math/tex">\mu = EX_n</script>,then</p>

<script type="math/tex; mode=display">\bar{X} \to \mu \quad \text{in almost sure}</script>

<ul>
  <li><strong>Central Limit Theorem</strong></li>
</ul>

<p>If <script type="math/tex">X_n, n=1,2,\dots,</script> are i.i.d, and <script type="math/tex">\mu = EX_n</script>, <script type="math/tex">\sigma^2 = var(X_n)</script>, and <script type="math/tex">% &lt;![CDATA[
0 < \sigma^2 < \infty %]]&gt;</script>, then</p>

<script type="math/tex; mode=display">\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \sim N(0,1)</script>

<ul>
  <li><strong>Delta Method</strong></li>
</ul>

<p>Let $Y_n$ be a sequence of random variables that satisfies <script type="math/tex">\sqrt{n}(Y_n-\theta) \sim N(0,\sigma^2)</script>
. For a given function $g$ and a specific value of $\theta$, suppose that <script type="math/tex">g'(\theta)</script> exists and is not 0. Then</p>

<script type="math/tex; mode=display">\sqrt{n} [g(Y_n)-g(\theta)] \sim N(0,\sigma^2[g'(\theta)]^2)</script>

<h2 id="sample-mean-and-sample-variance">Sample mean and Sample variance</h2>

<p>The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by</p>

<script type="math/tex; mode=display">\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i</script>

<p>The sample variance is the statistic defined by</p>

<script type="math/tex; mode=display">S^2 = \frac{1}{n-1} \sum_{i=1}{n}(X_i-\bar{X})^2</script>

<p>Let <script type="math/tex">X_1,\dots,X_n</script> be a random sample from a population with mean <script type="math/tex">\mu</script> and variance <script type="math/tex">% &lt;![CDATA[
\sigma^2 < \infty %]]&gt;</script>. Then</p>

<ol>
  <li><script type="math/tex">E\bar{X} = \mu</script>.</li>
  <li><script type="math/tex">Var \bar{X} = \frac{\sigma^2}{n}</script>.</li>
  <li><script type="math/tex">ES^2 = \sigma^2</script>.</li>
</ol>

<h3 id="normal-case">Normal Case</h3>

<p>If $X_n$ are independently and normally distributed <script type="math/tex">N(\mu,\sigma^2)</script>, then</p>

<ul>
  <li><script type="math/tex">\bar{X}_n \sim N(\mu,\sigma^2/n)</script>.</li>
  <li><script type="math/tex">(n-1)S_n^2 / \sigma^2 \sim \chi_{n-1}^2</script>.</li>
  <li><script type="math/tex">\bar{X}_n ~ \text{and} ~ S^2_n ~ \text{are independent}</script>.</li>
</ul>

<h3 id="non-normal-case">Non-normal Case</h3>

<p>When $X_i$ are not normal, the statistic is still commonly used because</p>

<ul>
  <li>Central Limit Theorem</li>
</ul>

<script type="math/tex; mode=display">\bar{X}_n \sim N(\mu,\sigma^2/n)</script>

<ul>
  <li>Law of Large Numbers</li>
</ul>

<script type="math/tex; mode=display">S_n^2 \to \sigma^2</script>

<ul>
  <li>Slusky’s Theorem</li>
</ul>

<script type="math/tex; mode=display">\frac{\bar{X}_n-\mu}{S_n/\sqrt{n}} = \frac{\sigma}{S_n} \cdot \frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}} \to N(0,1)</script>

]]></content>
  </entry>
  
</feed>
