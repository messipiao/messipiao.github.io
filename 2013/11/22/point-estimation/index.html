<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Point Estimation | Piao&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="When sampling is from a population described by a pdf or pmf $f(x \vert \theta)$, knowledge of $\theta$ yields knowledge of the entire population. Hence, it is natural to seek a method of finding a go">
<meta property="og:type" content="article">
<meta property="og:title" content="Point Estimation">
<meta property="og:url" content="http://messipiao.github.io/2013/11/22/point-estimation/index.html">
<meta property="og:site_name" content="Piao's Blog">
<meta property="og:description" content="When sampling is from a population described by a pdf or pmf $f(x \vert \theta)$, knowledge of $\theta$ yields knowledge of the entire population. Hence, it is natural to seek a method of finding a go">
<meta property="og:updated_time" content="2015-11-27T11:04:26.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Point Estimation">
<meta name="twitter:description" content="When sampling is from a population described by a pdf or pmf $f(x \vert \theta)$, knowledge of $\theta$ yields knowledge of the entire population. Hence, it is natural to seek a method of finding a go">
  
    <link rel="alternative" href="/atom.xml" title="Piao&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://cdn1.iconfinder.com/data/icons/ninja-things-1/720/ninja-background-128.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">CHEN PIAO</a></h1>
		</hgroup>

		
		<p class="header-subtitle">Step by Step</p>
		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>Menu</li>
						<li>Tags</li>
						
						
						<li>About Me</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">home</a></li>
				        
							<li><a href="/tags/Life">life</a></li>
				        
							<li><a href="/archives">archives</a></li>
				        
							<li><a href="/about">cv</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="weibo" target="_blank" href="http://weibo.com/p/1005051660361312" title="weibo">weibo</a>
					        
								<a class="twitter" target="_blank" href="https://twitter.com/messipiao" title="twitter">twitter</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/Analysis/" style="font-size: 10px;">Analysis</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/Probability/" style="font-size: 10px;">Probability</a> <a href="/tags/Programming/" style="font-size: 10px;">Programming</a> <a href="/tags/Research/" style="font-size: 10px;">Research</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">关于我，你知道什么？</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">CHEN PIAO</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="https://cdn1.iconfinder.com/data/icons/ninja-things-1/720/ninja-background-128.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">CHEN PIAO</h1>
			</hgroup>
			
			<p class="header-subtitle">Step by Step</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">home</a></li>
		        
					<li><a href="/tags/Life">life</a></li>
		        
					<li><a href="/archives">archives</a></li>
		        
					<li><a href="/about">cv</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="weibo" target="_blank" href="http://weibo.com/p/1005051660361312" title="weibo">weibo</a>
			        
						<a class="twitter" target="_blank" href="https://twitter.com/messipiao" title="twitter">twitter</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-point-estimation" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2013/11/22/point-estimation/" class="article-date">
  	<time datetime="2013-11-22T11:31:00.000Z" itemprop="datePublished">2013-11-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Point Estimation
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Statistics/">Statistics</a></li></ul>
	</div>

        

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>When sampling is from a population described by a pdf or pmf $f(x \vert \theta)$, knowledge of $\theta$ yields knowledge of the entire population. Hence, it is natural to seek a method of finding a good estimator of the point $\theta$, that is, a good point estimator. Note that an estimator is a function of the sample while an estimate is the realized value of an estimator (that is, a number) that is obtained when a sample is actually taken.</p>
<h2 id="Methods_of_Finding_Estimators">Methods of Finding Estimators</h2><h3 id="Method_of_Moments">Method of Moments</h3><p>Let $X_1,\dots,X_n$ be a sample from a population with pdf or pmf $f(x \vert \theta_1,\dots,\theta_k)$. Method of moments estimators are found by equating the first k sample moments to the corresponding k population moments.</p>
<p>$m_k = \frac{1}{n} \sum_{i=1}^{n}X_i^k = \mu’_k = EX^k$</p>
<h3 id="Maximum_Likelihood_Estimators">Maximum Likelihood Estimators</h3><p>The likelihood function is defined by</p>
<p>$L(\mathbf{\theta} \vert \mathbf{x}) = L(\theta_1,\dots,\theta_k \vert x_1,\dots,x_n) = \prod_{i=1}^{n} f(x_i \vert \theta_1,\dots,\theta_k)$</p>
<p>Basicly, we can solve the first derivative of <em>log likelihood function</em> to get the MLEs. If the likelihood function cannot be maximized analytically, it may be possible to use a computer and maximize the likelihood function numerically.</p>
<p>MLE has the following properties.</p>
<ul>
<li><strong>(Invariance property of MLEs)</strong> </li>
</ul>
<p>If $\hat{\theta}$ is the MLE of $\theta$, then for any function $f(\theta)$, the MLE of $f(\theta)$ is $f(\hat{\theta})$.</p>
<ul>
<li><strong>(Asymptotic Normality)</strong> </li>
</ul>
<p>Under appropriate regularity conditions,</p>
<p>$\sqrt{I_n(\theta_0)} (\hat{\theta}_n-\theta_0) \sim N(0,1)$</p>
<p>In addition</p>
<p>$\sqrt{I_n(\hat{\theta}_n)} (\hat{\theta}_n-\theta_0) \sim N(0,1)$</p>
<p>where $I(\theta)$ is called the information number or <em>Fisher information</em> of the sample.</p>
<p>$ I(\theta) = E[(\frac{\partial}{\partial \theta} \log f(X;\theta))^2] = -E[\frac{\partial^2}{\partial \theta^2} \log f(X;\theta)]$</p>
<p>Fisher information has following properties.</p>
<ol>
<li>$ I_{X,Y}(\theta) = I_X(\theta)+I_Y(\theta)$</li>
<li>$ I_n(\theta) = n I(\theta) $</li>
</ol>
<a id="more"></a>
<h3 id="The_EM_Algorithm">The EM Algorithm</h3><p>The EM algorithm is used to find <strong>MLEs</strong>, which is guaranteed to converge to the MLE. It is particularly suited to “missing data” problems, as the very fact that there are missing data can sometimes make calculations cumbersome.</p>
<p>If $\mathbf{Y} = (Y_1,\dots,Y_N)$ are the incomplete data, and $\mathbf{X} = (X_1,\dots,X_m)$ are the augmented data (missing data), making $(\mathbf{Y},\mathbf{X})$ the complete data. The densities $g(\cdot \vert \theta)$ of $\mathbf{Y}$ and $f(\cdot \vert \theta)$ of $\mathbf{Y},\mathbf{X}$ have the relationship</p>
<p>$g(\mathbf{y} \vert \theta) = \int f(\mathbf{y},\mathbf{x} \vert \theta) dx$</p>
<p>If we turn these into the likelihoods, $L(\theta \vert \mathbf{y}) = g(\mathbf{y} \vert \theta)$ is the incomplete-data likelihood and $L(\theta \vert \mathbf{y},\mathbf{x}) = f(\mathbf{y},\mathbf{x} \vert \theta)$ is the complete-data likelihood.i</p>
<p>Define</p>
<p>$k(\mathbf{x} \vert \theta,\mathbf{y}) = \frac{f(\mathbf{y},\mathbf{x} \vert \theta)}{g(\mathbf{y} \vert \theta)} = \frac{L(\theta \vert \mathbf{y},\mathbf{x})}{L(\theta \vert \mathbf{y})}$</p>
<p>Then we have</p>
<p>$\log L(\theta \vert \mathbf{y}) = \log L(\theta \vert \mathbf{y},\mathbf{x}) - \log k(\mathbf{x} \vert \theta,\mathbf{y})$</p>
<p>replace the right side with its expectation under $k(\mathbf{x} \vert \theta’,\mathbf{y})$, we have</p>
<p>$\log L(\theta \vert \mathbf{y}) = E[\log L(\theta \vert \mathbf{y},\mathbf{x}) \vert \theta’,\mathbf{y}] - E[\log k(\mathbf{x} \vert \theta,\mathbf{y}) \vert \theta’,\mathbf{y}]$</p>
<p>Now we start the algorithm: From an initial value $\theta^{(0)}$ we create a sequence $\theta^{(r)}$ according to</p>
<p>$\theta^{(r+1)} = \max_{\theta} E[\log L(\theta \vert \mathbf{y},\mathbf{x}) \vert \theta^{(r)},\mathbf{y}] $</p>
<h3 id="Bayes_Estimators">Bayes Estimators</h3><p>In Bayes approach $\theta$ is considered to be a quantity whose variation can be described by a probability distribution (called the prior distribution). This is a subjective distribution, based on the experimenter’s belief, and is formulated before the data are seen (hence the name prior distribution). A sample is then taken from a population indexed by $\theta$ and the prior distribution is updated with this sample information. The updated prior is called the posterior distribution. </p>
<p>If we denote the prior distribution by $\pi(\theta)$ and the sampling distribution by $f(x \vert \theta)$, then the posterior distribution, the conditional distribution of $\theta$ given the sample, $\mathbf{x}$, </p>
<p>$\pi(\theta \vert \mathbf{x}) = f(\mathbf{x} \vert \theta) \pi(\theta)/m(\mathbf{x})$</p>
<p>where $m(\mathbf{x})$ is the marginal distribution of $\mathbf{X}$, that is,</p>
<p>$m(\mathbf{x}) = \int f(\mathbf{x} \vert \theta) \pi(\theta) d \theta$</p>
<h2 id="Methods_of_Evaluating_Estimators">Methods of Evaluating Estimators</h2><h3 id="Mean_Squared_Error">Mean Squared Error</h3><blockquote>
<p>The mean square error (MSE) of an estimator W of a parameter $\theta$ is the function of $\theta$ defined by $E_{\theta}(W-\theta)^2$.</p>
</blockquote>
<p>Define</p>
<p>$Bias_{\theta}W = E_{\theta}W-\theta$</p>
<p>Then</p>
<p>$E_{\theta}(W-\theta)^2 = Var_{\theta}W+(E_{\theta}W-\theta)^2 = Var_{\theta}W+(Bias_{\theta}W)^2$</p>
<h3 id="Best_Unbiased_Estimators">Best Unbiased Estimators</h3><blockquote>
<p>An estimator W’ is a <em>best unbiased estimator of $\tau(\theta)$</em> if it satisfies $E_{\theta}W’ = \tau(\theta)$ for all $\theta$ and, for any other estimator W with $E_{\theta}W = \tau(\theta)$, we have $Var_{\theta}W’ \le Var_{\theta}W$ for all $\theta$. W’ is also called a <em>uniform minimum variance unbiased estimator</em> (<strong>UMVUE</strong>) of $\tau(\theta)$.</p>
</blockquote>
<ul>
<li><strong>Cramer-Rao Inequality</strong></li>
</ul>
<p>Let $X_1,\dots,X_n$ be a sample with pdf $f(x \vert \theta)$, and let $W(\mathbf{X}) = W(X_1,\dots,X_n)$ be <em>any estimator satisfying</em></p>
<p>$\frac{d}{d\theta} E_{\theta}W(\mathbf{X}) = \int \frac{\partial}{\partial \theta}[W(\mathbf{x})f(x \vert \theta)] dx$</p>
<p>and</p>
<p>$Var_{\theta}W(\mathbf{X}) &lt; \infty$</p>
<p>Then</p>
<p>$Var_{\theta}(W(\mathbf{X})) \ge \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{E_{\theta}[(\frac{\partial}{\partial \theta} \log f(\mathbf{X} \vert \theta))^2]} = \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{I_n(\theta)}$</p>
<p>when $X_1,\dots,X_n$ are <em>iid</em>, based on the property of <em>Fisher Information</em>, we have</p>
<p>$Var_{\theta}(W(\mathbf{X})) \ge \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{nE_{\theta}[(\frac{\partial}{\partial \theta} \log f(X \vert \theta))^2]} = \frac{(\frac{d}{d\theta}E_{\theta}W(\mathbf{X}))^2}{nI(\theta)}$</p>
<p>We need to verify whether the <strong>Cramer-Rao Lower Bound</strong> is attainable.</p>
<ul>
<li>Let $X_1,\dots,X_n$ be <em>iid</em> $f(x \vert \theta)$, which satisfies the conditions of the <em>Cramer-Rao Theorem</em>. Let $L(\theta \vert \mathbf{x}) = \prod_{i=1}^{n} f(x_i \vert \theta)$ denote the likelihood function. If $<br>W(\mathbf{X})=W(X_1,\dots,X_n) $ is any unbiased estimator of $\tau(\theta)$, then $W(\mathbf{X})$ attains the Cramer-Rao Lower Bound if and only if </li>
</ul>
<p>$a(\theta)[W(\mathbf{x})-\tau(\theta)] = \frac{\partial}{\partial \theta}\log L(\theta \vert \mathbf{x})$</p>
<p>for some function $a(\theta)$.</p>
<p>Two theorems are used to find the <strong>UMVUE</strong>.</p>
<ul>
<li><strong>Rao-Blackwell Theorem</strong> </li>
</ul>
<p>Let W be any unbiased estimator of $\tau(\theta)$, and let T be a sufficient statistic for $\theta$. Then $E(W \vert T)$ is a UMVUE.</p>
<ul>
<li><strong>Lehmann Scheffe Theorem</strong></li>
</ul>
<p>Suppose there exists a sufficient and complete statistic T for $\theta$. If there exisits unbiased estimator of $\tau(\theta)$, then the UMVUE takes the form of h(T), where h is a Borel function.</p>
<p>It is also noted that UMVUE is <strong>unique</strong>.</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2013/11/24/hypothesis-testing/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Hypothesis Testing
        
      </div>
    </a>
  
  
    <a href="/2013/11/22/data-reduction/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Data Reduction</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>






<section id="comments">
  <div id="disqus_thread"></div>
    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'messipiao'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2015 CHEN PIAO
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>